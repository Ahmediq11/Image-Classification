{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è 1. Environment Setup\n",
        "\n",
        "This first cell prepares our Python environment by installing the necessary libraries for our image classification task.\n",
        "\n",
        "- **`datasets`**: A Hugging Face library for easily downloading and working with datasets from the Hub.\n",
        "- **`accelerate`**: Optimizes PyTorch training across different hardware, making our training process more efficient.\n",
        "- **`evaluate`**: Provides a simple way to load and compute common machine learning metrics like accuracy.\n",
        "- **`warnings.filterwarnings('ignore')`**: This is used to suppress warning messages and keep the notebook output clean for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "# !pip install -U accelerate\n",
        "# !pip install evaluate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• 2. Loading the Dataset\n",
        "\n",
        "We use the `load_dataset` function from the `datasets` library to download the `AkshilShah21/food_images` dataset directly from the Hugging Face Hub. This dataset contains images of different types of food, already split into training and testing sets. We can then easily inspect an individual image from the dataset, which is stored as a PIL (Python Imaging Library) object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "food = load_dataset(\"AkshilShah21/food_images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["food['train'][0]['image'] # class id => 6"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üè∑Ô∏è 3. Creating Label Mappings\n",
        "\n",
        "Machine learning models work with numbers, not text labels like \"pizza\" or \"samosa\". Therefore, we need to create a mapping between the string labels and integer indices.\n",
        "\n",
        "- **`label2id`**: A dictionary that maps each food name (e.g., `'pizza'`) to a unique integer (e.g., `6`).\n",
        "- **`id2label`**: The inverse dictionary that maps each integer back to its corresponding food name.\n",
        "\n",
        "These mappings are crucial for configuring the model correctly and for interpreting its predictions later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = food['train'].features['label'].names\n",
        "label2id, id2label = dict(), dict()\n",
        "\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "print(label2id)\n",
        "print(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üñºÔ∏è 4. Image Preprocessing\n",
        "\n",
        "Before we can feed images to our model, they must be preprocessed. We load an `AutoImageProcessor` from the same checkpoint as our model (`google/vit-base-patch16-224-in21k`). This processor knows the exact requirements of the model, such as the expected image size and the specific mean and standard deviation values needed for normalization. This ensures our input data is formatted perfectly for the Vision Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "model_ckpt = \"google/vit-base-patch16-224-in21k\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_ckpt, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® 5. Data Augmentation and Transformation\n",
        "\n",
        "To make our model more robust and prevent overfitting, we apply **data augmentation**. This involves creating modified versions of our training images.\n",
        "\n",
        "We use `torchvision.transforms` to create a processing pipeline:\n",
        "1.  **`RandomResizedCrop`**: Randomly crops parts of the image and resizes them to the model's expected input size. This helps the model learn to recognize food from different angles and zoom levels.\n",
        "2.  **`ToTensor`**: Converts the PIL images into PyTorch tensors.\n",
        "3.  **`Normalize`**: Scales the pixel values using the mean and standard deviation from our image processor.\n",
        "\n",
        "This transformation pipeline is then applied to the dataset on-the-fly using `.with_transform()`, which is a memory-efficient way to process the data as it's needed during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "\n",
        "size = (\n",
        "    image_processor.size['shorted_edge']\n",
        "    if \"shorted_edge\" in image_processor.size\n",
        "    else (image_processor.size['height'], image_processor.size['width'])\n",
        ")\n",
        "\n",
        "\n",
        "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
        "\n",
        "def transforms(examples):\n",
        "    examples['pixel_values'] = [_transforms(img.convert('RGB')) for img in examples['image']]\n",
        "    del examples['image']\n",
        "\n",
        "    return examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["food = food.with_transform(transforms)\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà 6. Defining Evaluation Metrics\n",
        "\n",
        "To monitor our model's performance during training, we need to define a metric. We use the `evaluate` library to load the standard `accuracy` metric. We then create a `compute_metrics` function that takes the model's raw output (logits) and the true labels, finds the predicted class by selecting the logit with the highest value (`np.argmax`), and then compares the predictions to the true labels to calculate the accuracy. This function will be called by the `Trainer` at the end of each evaluation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy = evaluate.load('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ 7. Loading and Configuring the Model\n",
        "\n",
        "We are now ready to load our model. We use `AutoModelForImageClassification` to load the pre-trained Vision Transformer (`google/vit-base-patch16-224-in21k`). Crucially, we configure it for our specific task by:\n",
        "\n",
        "- Setting `num_labels` to the number of food classes in our dataset.\n",
        "- Providing our `id2label` and `label2id` mappings, so the model understands the connection between its output nodes and the food names.\n",
        "\n",
        "This process replaces the model's original classification head with a new, untrained one that is perfectly sized for our food dataset. We then move the model to the GPU (`cuda`) if available for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_ckpt,\n",
        "    num_labels = len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÇ 8. Training the Model\n",
        "\n",
        "We use the Hugging Face `Trainer` API to handle the entire training and evaluation process. First, we set up `TrainingArguments` to define all the hyperparameters for our training run, such as the learning rate, number of epochs, and batch size. We also specify that the model should be evaluated and saved at the end of each epoch, and that the best-performing model based on accuracy should be loaded at the end.\n",
        "\n",
        "Then, we instantiate the `Trainer`, passing it all the necessary components: our model, the training arguments, the train and test datasets, the image processor (which acts as a tokenizer for images), and our `compute_metrics` function. Finally, we call `trainer.train()` to begin the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir = \"train_dir\",\n",
        "    remove_unused_columns=False,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=4,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=food['train'],\n",
        "    eval_dataset=food['test'],\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["trainer.train()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ 9. Saving the Model\n",
        "\n",
        "After training is complete, the `Trainer` will have saved the best version of our fine-tuned model to the specified output directory. We can also save it manually to a clear, descriptive folder name like `food_classification` using the `trainer.save_model()` command. This saves the model's weights and configuration, allowing us to easily load it later for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["trainer.save_model('food_classification')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ 10. Inference with a Pipeline\n",
        "\n",
        "The easiest way to use our fine-tuned model for prediction is with a `pipeline`. We create an `image-classification` pipeline and point it to our saved model directory (`food_classification`). This pipeline handles all the necessary preprocessing steps automatically.\n",
        "\n",
        "We can then test it on a new image. Here, we download an image of a pizza from a URL, open it with the PIL library, and simply pass the image object to our pipeline to get a prediction. The pipeline returns the most likely food classes and their corresponding confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-classification\", model='food_classification', device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "url = 'https://www.indianhealthyrecipes.com/wp-content/uploads/2015/10/pizza-recipe-1.jpg'\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "image.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["pipe(image)"]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
